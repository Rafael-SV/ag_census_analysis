{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Python Spark SQL basic example') \\\n",
    "    .config('spark.some.config.option', 'some-value') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.max_rows', 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2002 = spark.read.parquet('census_02.parquet')\n",
    "#df_2007 = spark.read.load('census_07.parquet')\n",
    "#df_2012 = spark.read.load('census_12.parquet')\n",
    "#df_2002.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2002 = spark.read.parquet(\"census_02.parquet\")\n",
    "\n",
    "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "df_2002.createOrReplaceTempView(\"df_2002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GROUP_DESC,', 'COMMODITY_DESC,', 'CLASS_DESC,', 'PRODN_PRACTICE_DESC,', 'UTIL_PRACTICE_DESC,', 'STATISTICCAT_DESC,', 'UNIT_DESC,', 'SHORT_DESC,', 'DOMAIN_DESC,', 'VALUE']\n"
     ]
    }
   ],
   "source": [
    "interesting_column_str = 'GROUP_DESC, COMMODITY_DESC, CLASS_DESC, PRODN_PRACTICE_DESC, UTIL_PRACTICE_DESC, STATISTICCAT_DESC, UNIT_DESC, \\\n",
    "SHORT_DESC, DOMAIN_DESC, VALUE'\n",
    "interesting_column_list = str.split(interesting_column_str)\n",
    "print(interesting_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_crops_2002_df = spark.sql(\"SELECT {} FROM parquetFile WHERE SECTOR_DESC='CROPS'\".format(interesting_column_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unit_desc_types = spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only_crops_2002_df.toPandas().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
